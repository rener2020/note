---
title: 强化学习
tags: 新建,模板,小书匠
category: /小书匠/日记/2021-12
renderNumberedHeading: true
grammar_cjkRuby: true
---

# Terminology
## state and action
- state $s$ (this frame)
- action a $\in$ {left}
- policy $\pi$
	- policy function $\pi:(s,a) \to [0,1]$:
		- $\pi(a|s)=\mathbb{P}(A=a|S=s)$
	- it is the probability of taking action $A=a$ given state $s$, e.g.,
		- $\pi(left|s)=0.2$
		- $\pi(right|s)=0.2$
		- $\pi(up|s)=0.6$
	- upon observing state $S = s$ ,the agent's action A can be random
- reward $R$
	- Collect a coin : $R = +1$
	- Win a game: $R=+10000$
	- Loss a  game:$R=-10000$
	- Nothing happens:$R=0$
- state transision
	- state transision can be random
	- Radomness is from the enviorment
	- $p(s'|s,a) = \mathbb{P}(S'=s'|S=s,A=a)$

## agent environment interaction
![![enter description here](./images/1640243067718.png)](./images/1640243418748.png)
- actions have randomness
	- Given state $s$, the action can be random,e.g.,
		- $\pi(left|s)=0.2$
		- $\pi(right|s)=0.2$
		- $\pi(up|s)=0.6$
- State transitions have ramdomness.
	- Given state $S=s$ and action $A=a$, the environment randomly generates a new state $S'$

## Playing the game  using AI
- observe a frame (state s_1)
- Make action a_1
- observe a new frame (state s_2) and reward r_1
- make action a_2
- ...

(state, action, reward) trajectory
$(s_1,a_1,r_1) ... (s_T,a_T,r_T)$

## Return
Definition: Return (aka cumulative future reward).
- $U_t=R_t+R_{t+1}+...$
	Are $R_t$ and $R_{t+1}$ equally important?

Definition: Discounted return (aka cumlative discounted future reward).
- $\gamma$:discount rate (tuning hyper-parameter)
- $U_t = R_t + \gamma{}R_{t+1}+...+ \gamma^tR_{t}+...$

At time step t, the return U_t is random.
- Two sources of randomness:
	- Action can be random:$\mathbb{P}[A=a|S=s] = \pi(a|s)$
	- New state can be random: $\mathbb{P}[S'=s'|S=s,A=a] = p(s'|s,a)$
- for any $i\geq t$, the reward $R_i$ depends on $S_i$ and $A_i$
- Thus, given $s_t$, the return $U_t$ depends on the random variabels:
	- $A_t,A_{t+1},A_{t+2},\cdots \   and \ S_{t+1},S_{t+2},\cdots .$

## Value Functions
### Action-Value Function $Q(s,a)$
Definition: Discounted return (aka cumulative discounted futrure reward).
- $U_t=R_t+\gamma{}R_{t+1}+\cdots$ 
	只是一个随机变量，在$t$时刻我们不知道其数值

Definition: Action-Value function for policy $\pi$
- $Q_\pi{}(s_t,a_t)=\mathbb{E}[U_t|S_t=s_t,A_t=a_t]$
- return $U_t$ depends on actions $A_t,A_{t+1},\cdots$ and states $S_t,S_{t+1},\cdots$